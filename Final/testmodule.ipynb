{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import jamotools\n",
    "from collections import Counter\n",
    "np.random.seed(42)\n",
    "import pickle\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "\n",
    "dependencies = {\n",
    "    'precision': precision,\n",
    "    'recall' : recall,\n",
    "    'f1score' : f1score,\n",
    "  }\n",
    "  \n",
    "class dataset():\n",
    "  def __init__(self, dataset, train_dataset, max_fea = None, max_len = None):\n",
    "    self.sentences = [sentence for sentence in dataset['text']]\n",
    "    self.labels = [label for label in dataset['label']]\n",
    "    self.korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "    self.vocab = self.get_vocab(max_fea)\n",
    "    self.vocab_size = len(self.vocab)\n",
    "    self.word2idx = {word : index for index, (word, count) in enumerate(self.vocab)}\n",
    "    self.idx2word = {index : word for index, (word, count) in enumerate(self.vocab)}\n",
    "    if max_len == None: \n",
    "      self.max_len = self.find_max_len()\n",
    "    else : \n",
    "      self.max_len = max_len\n",
    "    self.max_feature = max_fea\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return (self.preprocess_sentence(self.sentences[index]), self.labels[index])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "  def get_vocab(self, max_fea):\n",
    "    \n",
    "\n",
    "    return train_dataset.get_vocab(max_fea)\n",
    "\n",
    "  def jamo(self, sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "      chars = self.korean.sub('', jamotools.split_syllables(sentence))\n",
    "      result.append(list(chars))\n",
    "    \n",
    "    return result\n",
    "\n",
    "  def jamochar(self, char):\n",
    "    char = self.korean.sub('', jamotools.split_syllables(char)) \n",
    "    return char\n",
    "\n",
    "  def preprocess_sentence(self, sentence):\n",
    "    result = []\n",
    "    padding = '<PAD>'\n",
    "    if len(sentence) > self.max_len:\n",
    "      fixed_sen = ['<SOS>']\n",
    "      fixed_sen += list(sentence[:self.max_len])\n",
    "      fixed_sen += ['<EOS>']\n",
    "    else:\n",
    "      fixed_sen = ['<SOS>']\n",
    "      fixed_sen += list(sentence) \n",
    "      fixed_sen += ['<EOS>']\n",
    "      while len(fixed_sen) != self.max_len + 2: \n",
    "        fixed_sen += [padding] \n",
    "    \n",
    "    for char in fixed_sen:\n",
    "      if char in ['<SOS>', '<EOS>']:\n",
    "        result.append(self.word2idx[char])\n",
    "      elif char == '<PAD>':\n",
    "        result+=([self.word2idx['<PAD>']] * 3)\n",
    "      else:  \n",
    "        sep_char = self.jamochar(char)\n",
    "        if len(sep_char) == 2 : \n",
    "          result += ([self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]+[self.word2idx['<PAD>']])\n",
    "        elif len(sep_char) == 1:\n",
    "          result += ([self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]+([self.word2idx['<PAD>']]*2))\n",
    "        elif sep_char == '.' or sep_char == ' ':\n",
    "          result += ([self.word2idx['<PAD>']]*3)\n",
    "        else : \n",
    "          result += [self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]\n",
    "        \n",
    "      \n",
    "    self.padding(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "  def padding(self, result):\n",
    "    length = 3 * (self.max_len) + 2\n",
    "    while len(result) < length :\n",
    "      result.append(0)\n",
    "                   \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  def find_max_len(self):\n",
    "    return max(len(item) for item in self.sentences)  \n",
    "\n",
    "\n",
    "class dataset_train():\n",
    "  def __init__(self, dataset, max_fea = None, max_len = None):\n",
    "    self.sentences = [sentence for sentence in dataset['text']]\n",
    "    self.labels = [label for label in dataset['label']]\n",
    "    self.korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "    self.vocab = self.get_vocab(max_fea)\n",
    "    self.vocab_size = len(self.vocab)\n",
    "    self.word2idx = {word : index for index, (word, count) in enumerate(self.vocab)}\n",
    "    self.idx2word = {index : word for index, (word, count) in enumerate(self.vocab)}\n",
    "    if max_len == None: \n",
    "      self.max_len = self.find_max_len()\n",
    "    else : \n",
    "      self.max_len = max_len\n",
    "    self.max_feature = max_fea\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return (self.preprocess_sentence(self.sentences[index]), self.labels[index])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "  def get_vocab(self, max_fea):\n",
    "    initial_words = ['<EOS>', '<SOS>','<UNK>','<PAD>'] # 순서 반대로 들어감 ! Pad 가 index 0!\n",
    "    counter = Counter()\n",
    "    for char in self.jamo(self.sentences):\n",
    "        counter.update(char)\n",
    "  \n",
    "    \n",
    "    if max_fea == None:\n",
    "      max_fea = len(counter.keys())\n",
    "\n",
    "    vocab_words = counter.most_common(max_fea)\n",
    "\n",
    "    for initial_word in initial_words:\n",
    "      vocab_words.insert(0, (initial_word, 0))\n",
    "\n",
    "    return vocab_words\n",
    "\n",
    "  def jamo(self, sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "      chars = self.korean.sub('', jamotools.split_syllables(sentence))\n",
    "      result.append(list(chars))\n",
    "    \n",
    "    return result\n",
    "\n",
    "  def jamochar(self, char):\n",
    "    char = self.korean.sub('', jamotools.split_syllables(char)) \n",
    "    return char\n",
    "\n",
    "  def preprocess_sentence(self, sentence):\n",
    "    result = []\n",
    "    padding = '<PAD>'\n",
    "    if len(sentence) > self.max_len:\n",
    "      fixed_sen = ['<SOS>']\n",
    "      fixed_sen += list(sentence[:self.max_len])\n",
    "      fixed_sen += ['<EOS>']\n",
    "    else:\n",
    "      fixed_sen = ['<SOS>']\n",
    "      fixed_sen += list(sentence) \n",
    "      fixed_sen += ['<EOS>']\n",
    "      while len(fixed_sen) != self.max_len + 2: \n",
    "        fixed_sen += [padding] \n",
    "    \n",
    "    for char in fixed_sen:\n",
    "      if char in ['<SOS>', '<EOS>']:\n",
    "        result.append(self.word2idx[char])\n",
    "      elif char == '<PAD>':\n",
    "        result+=([self.word2idx['<PAD>']] * 3)\n",
    "      else:  \n",
    "        sep_char = self.jamochar(char)\n",
    "        if len(sep_char) == 2 : \n",
    "          result += ([self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]+[self.word2idx['<PAD>']])\n",
    "        elif len(sep_char) == 1:\n",
    "          result += ([self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]+([self.word2idx['<PAD>']]*2))\n",
    "        elif sep_char == '.' or sep_char == ' ':\n",
    "          result += ([self.word2idx['<PAD>']]*3)\n",
    "        else : \n",
    "          result += [self.word2idx[x] if x in self.word2idx.keys() else self.word2idx['<UNK>'] for x in sep_char]\n",
    "        \n",
    "      \n",
    "    self.padding(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "  def padding(self, result):\n",
    "    length = 3 * (self.max_len) + 2\n",
    "    while len(result) < length :\n",
    "      result.append(0)\n",
    "                   \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  def find_max_len(self):\n",
    "    return max(len(item) for item in self.sentences)  \n",
    "\n",
    "with open(os.path.abspath('./train_dataset.p'), 'rb') as file:    \n",
    "\n",
    "   train_dataset = pickle.load(file)\n",
    "\n",
    "def yok_classifier_lime(sentence):\n",
    "  \n",
    "  print(train_dataset)\n",
    "  sentence_dataframe= pd.DataFrame()\n",
    "  sentence_dataframe['text'] = sentence  \n",
    "  sentence_dataframe['label'] = [1] * len(sentence)\n",
    "  sentence_dataset = dataset(sentence_dataframe, train_dataset, max_fea = 5000, max_len = 10)\n",
    "  input_data =np.zeros((len(sentence), 32))\n",
    "  for i in range(32):\n",
    "    for j in range(len(sentence)):\n",
    "      input_data[j][i] = sentence_dataset[j][0][i]\n",
    "  return np.array([[float(1-x), float(x)] for x in model_1.predict(input_data)])\n",
    "\n",
    "def final_yok_classifing(sentence):\n",
    "  class_names = ['욕설이 아님', '욕설']\n",
    "  explainer = LimeTextExplainer(class_names=class_names)\n",
    "  exp = explainer.explain_instance(sentence[0],yok_classifier_lime, num_features = 100)\n",
    "  return exp.as_list()\n",
    "\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class ClovaSpeechClient:\n",
    "    # Clova Speech invoke URL\n",
    "    invoke_url = 'https://clovaspeech-gw.ncloud.com/external/v1/1656/415473a357d9d696222e86407a96e6a0e022269c22dd2c8d5187a69c100f0097'\n",
    "    # Clova Speech secret key\n",
    "    secret = 'ba374fcca14d4ea7aed6e23911396516'\n",
    "\n",
    "    def req_url(self, url, completion, callback=None, userdata=None, forbiddens=None, boostings=None, wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'url': url,\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'Content-Type': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        return requests.post(headers=headers,\n",
    "                             url=self.invoke_url + '/recognizer/url',\n",
    "                             data=json.dumps(request_body).encode('UTF-8'))\n",
    "\n",
    "    def req_object_storage(self, data_key, completion, callback=None, userdata=None, forbiddens=None, boostings=None,\n",
    "                           wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'dataKey': data_key,\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'Content-Type': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        return requests.post(headers=headers,\n",
    "                             url=self.invoke_url + '/recognizer/object-storage',\n",
    "                             data=json.dumps(request_body).encode('UTF-8'))\n",
    "\n",
    "    def req_upload(self, file, completion, callback=None, userdata=None, forbiddens=None, boostings=None,\n",
    "                   wordAlignment=True, fullText=True, diarization=None):\n",
    "        request_body = {\n",
    "            'language': 'ko-KR',\n",
    "            'completion': completion,\n",
    "            'callback': callback,\n",
    "            'userdata': userdata,\n",
    "            'wordAlignment': wordAlignment,\n",
    "            'fullText': fullText,\n",
    "            'forbiddens': forbiddens,\n",
    "            'boostings': boostings,\n",
    "            'diarization': diarization,\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json;UTF-8',\n",
    "            'X-CLOVASPEECH-API-KEY': self.secret\n",
    "        }\n",
    "        print(json.dumps(request_body, ensure_ascii=False).encode('UTF-8'))\n",
    "        files = {\n",
    "            'media': open(file, 'rb'),\n",
    "            'params': (None, json.dumps(request_body, ensure_ascii=False).encode('UTF-8'), 'application/json')\n",
    "        }\n",
    "        response = requests.post(headers=headers, url=self.invoke_url + '/recognizer/upload', files=files)\n",
    "        return response\n",
    "\n",
    "\n",
    "# res = ClovaSpeechClient().req_url(url='http://example.com/media.mp3', completion='sync')\n",
    "# res = ClovaSpeechClient().req_object_storage(data_key='data/media.mp3', completion='sync')\n",
    "\n",
    "def STT(file_dir):\n",
    "    res = ClovaSpeechClient().req_upload(file= file_dir, completion='sync')\n",
    "    result_ = res.text\n",
    "    rw = re.compile('\"words\":')\n",
    "    rt = re.compile(',\"textEdited\"')\n",
    "    sw = re.compile(',\"text\":\"')\n",
    "    st = re.compile('\",\"confidence\"')\n",
    "    iw = [m.end() for m in rw.finditer(result_)]\n",
    "    it = [m.start() for m in rt.finditer(result_)]\n",
    "    siw = [m.end() for m in sw.finditer(result_)]\n",
    "    sit = [m.start() for m in st.finditer(result_)]\n",
    "\n",
    "    stt_sentence = [result_[siw[i]:sit[i]] for i in range(len(siw))]\n",
    "    del stt_sentence[-1]\n",
    "    \n",
    "\n",
    "    result = [result_[iw[i]:it[i]] for i in range(len(iw))]\n",
    "    result = \",\".join(result)\n",
    "    result = result.replace('[','', -1)\n",
    "    result = result.replace(']','', -1)\n",
    "    result = result.replace('\"','', -1)\n",
    "    result = result.split(\",\")\n",
    "    time_stemp = [[int(result[3*i]), int(result[3*i +1])] for i in range(len(result)//3)]\n",
    "    stt_word = [result[3*i + 2] for i in range(len(result)//3)]\n",
    "\n",
    "\n",
    "    return time_stemp, stt_word, stt_sentence\n",
    "\n",
    "\n",
    "def get_swear(ts, stt, stt_sentence):\n",
    "  res_sen =[]\n",
    "  res_sen_temp=[]\n",
    "  swear_ts = []\n",
    "  split_number = 3\n",
    "  for index, word in enumerate(stt):\n",
    "    if index !=0 and index % split_number== 0:\n",
    "      res_sen.append(res_sen_temp)\n",
    "      res_sen_temp =[]\n",
    "    if index == len(stt) -1:\n",
    "      res_sen_temp.append(word)\n",
    "      res_sen.append(res_sen_temp)\n",
    "      break\n",
    "    res_sen_temp.append(word)\n",
    "  res_sen_str = []\n",
    "  for i in range(len(res_sen)):\n",
    "    res_sen_str.append(' '.join(res_sen[i]))\n",
    "  lime_sen_result =[]\n",
    "  lime_word_result = []\n",
    "  for i in range(len(res_sen_str)):\n",
    "    lime_sen_result.append(yok_classifier_lime([res_sen_str[i]]))\n",
    "    lime_word_result.append(final_yok_classifing([res_sen_str[i]]))\n",
    "  lime_word_result = sum(lime_word_result, [])\n",
    "  lime_word_result_0 = [item[0] for item in lime_word_result]\n",
    "  # print(lime_word_result)\n",
    "  # print(lime_word_result_0)\n",
    "  # print(lime_sen_result)\n",
    "  \n",
    "  for index, word in enumerate(stt):\n",
    "    word = word.replace('.', '', -1)\n",
    "    sen_res = lime_sen_result[index // split_number][0][1]\n",
    "    if lime_word_result[lime_word_result_0.index(word)][1] > 0.9 - sen_res:\n",
    "      swear_ts.append(ts[index])\n",
    "      print(word)\n",
    "  return swear_ts\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "def create_beep(duration):\n",
    "    sps = 44100\n",
    "    freq_hz = 1000.0\n",
    "    vol = 0.1\n",
    "\n",
    "    esm = np.arange(duration / 1000 * sps)\n",
    "    wf = np.sin(2 * np.pi * esm * freq_hz / sps)\n",
    "    wf_quiet = wf * vol\n",
    "    wf_int = np.int16(wf_quiet * 32767)\n",
    "\n",
    "    beep = AudioSegment(\n",
    "        wf_int.tobytes(), \n",
    "        frame_rate=sps,\n",
    "        sample_width=wf_int.dtype.itemsize, \n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    return beep\n",
    "\n",
    "\n",
    "\n",
    "def using_beautiful_word(file_dir, format = 'mp3'):\n",
    "  global ts, stt, stt_sen\n",
    "  ts, stt, stt_sen= STT(file_dir)\n",
    "  swear_ts = get_swear(ts, stt, stt_sen)\n",
    "  sound = AudioSegment.from_mp3(file_dir)\n",
    "  mixed_final = sound\n",
    "\n",
    "  for i in range(len(swear_ts)):\n",
    "     beep = create_beep(duration=swear_ts[i][1] - swear_ts[i][0])\n",
    "     mixed_final = mixed_final.overlay(beep, position=swear_ts[i][0], gain_during_overlay=-50)\n",
    "\n",
    "  return mixed_final\n",
    "\n",
    "from moviepy.editor import * \n",
    "import os\n",
    "\n",
    "def generate_output(video_dir, working_path):\n",
    "  videoclip = VideoFileClip(video_dir)\n",
    "  o_audio_dir = os.path.join(working_path + '/origin.mp3')\n",
    "  n_audio_dir = os.path.join(working_path + '/new.mp3')\n",
    "  videoclip.audio.write_audiofile(o_audio_dir)\n",
    "  using_beautiful_word(o_audio_dir).export(n_audio_dir, format = \"mp3\")\n",
    "  audioclip = AudioFileClip(n_audio_dir)\n",
    "\n",
    "  videoclip.audio = audioclip\n",
    "  return videoclip\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    \n",
    "    return _f1score\n",
    "def final_output(model_dir, video_dir, output_dir, working_path):\n",
    "  global model_1\n",
    "  dependencies = {\n",
    "    'precision': precision,\n",
    "    'recall' : recall,\n",
    "    'f1score' : f1score,\n",
    "  }\n",
    "\n",
    "  model_1 = tf.keras.models.load_model(model_dir\n",
    "                                      , custom_objects=dependencies)\n",
    "  final_video_output = generate_output(video_dir, working_path)\n",
    "  final_video_output.write_videofile(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  34%|███▎      | 78/232 [00:00<00:00, 763.34it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /Users/2_eu_genie/Desktop/workingspace/grad_web/workingspace/origin.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "b'{\"language\": \"ko-KR\", \"completion\": \"sync\", \"callback\": null, \"userdata\": null, \"wordAlignment\": true, \"fullText\": true, \"forbiddens\": null, \"boostings\": null, \"diarization\": null}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  34%|███▍      | 79/233 [00:00<00:00, 789.16it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/2_eu_genie/Desktop/workingspace/grad_web/outputs/outputvideo.mp4.\n",
      "MoviePy - Writing audio in outputvideoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   3%|▎         | 8/315 [00:00<00:03, 77.69it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /Users/2_eu_genie/Desktop/workingspace/grad_web/outputs/outputvideo.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/2_eu_genie/Desktop/workingspace/grad_web/outputs/outputvideo.mp4\n"
     ]
    }
   ],
   "source": [
    "origin_file = \"/Users/2_eu_genie/Desktop/workingspace/grad_web/IMG_3182.MOV\"\n",
    "model_dir = os.path.abspath('./final_model.h5')\n",
    "\n",
    "video_dir = os.path.abspath('./uploads/inputvideo.mp4')\n",
    "output_dir = os.path.abspath('./outputs/outputvideo.mp4')\n",
    "working_path = os.path.abspath('./workingspace')\n",
    "final_output(model_dir, video_dir, output_dir, working_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이 영상은 졸업 작품 테스트 영상입니다',\n",
       " '영상에 욕설이 일부 포함되어 있습니다. 씨발 이 새끼 때문에 내가 며칠 밤을 새는 거야. 씨발 너무 힘들다.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swear(ts, stt, stt_sentence):\n",
    "  res_sen =[]\n",
    "  res_sen_temp=[]\n",
    "  swear_ts = []\n",
    "  split_number = 3\n",
    "  for index, word in enumerate(stt):\n",
    "    if index !=0 and index % split_number== 0:\n",
    "      res_sen.append(res_sen_temp)\n",
    "      res_sen_temp =[]\n",
    "    if index == len(stt) -1:\n",
    "      res_sen_temp.append(word)\n",
    "      res_sen.append(res_sen_temp)\n",
    "      break\n",
    "    res_sen_temp.append(word)\n",
    "  res_sen_str = []\n",
    "  for i in range(len(res_sen)):\n",
    "    res_sen_str.append(' '.join(res_sen[i]))\n",
    "  lime_sen_result =[]\n",
    "  lime_word_result = []\n",
    "  for i in range(len(res_sen_str)):\n",
    "    lime_sen_result.append(yok_classifier_lime([res_sen_str[i]]))\n",
    "    lime_word_result.append(final_yok_classifing([res_sen_str[i]]))\n",
    "  lime_word_result = sum(lime_word_result, [])\n",
    "  lime_word_result_0 = [item[0] for item in lime_word_result]\n",
    "  # print(lime_word_result)\n",
    "  # print(lime_word_result_0)\n",
    "  # print(lime_sen_result)\n",
    "  \n",
    "  for index, word in enumerate(stt):\n",
    "    word = word.replace('.', '', -1)\n",
    "    sen_res = lime_sen_result[index // split_number][0][1]\n",
    "    if lime_word_result[lime_word_result_0.index(word)][1] > 0.9 - sen_res:\n",
    "      swear_ts.append(ts[index])\n",
    "      print(word)\n",
    "  return swear_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n",
      "[('영상은', -0.32466545847290684), ('졸업', 0.19911085138483123), ('이', -0.004919830464285534), ('테스트', 0.46342275718694786), ('영상입니다', -0.1955167702307826), ('작품', -0.08382445321826106), ('영상에', -0.07211495008416743), ('일부', -0.029534727544589422), ('욕설이', 0.005380815404063237), ('씨발', 0.915685357475486), ('있습니다', 0.04951966591671664), ('포함되어', 0.03254138192323529), ('새끼', 0.39825896871163036), ('때문에', -0.2672615462521745), ('이', 0.2063322279411072), ('며칠', -0.2254250299732428), ('밤을', -0.19920839450890715), ('내가', -0.14298425714368626), ('씨발', 0.5654777036219274), ('거야', -0.15817838957418084), ('새는', 0.021108095550835593), ('힘들다', -0.030802230788461914), ('너무', 0.024146162364826623)]\n",
      "['영상은', '졸업', '이', '테스트', '영상입니다', '작품', '영상에', '일부', '욕설이', '씨발', '있습니다', '포함되어', '새끼', '때문에', '이', '며칠', '밤을', '내가', '씨발', '거야', '새는', '힘들다', '너무']\n",
      "[array([[0.92974842, 0.07025161]]), array([[0.44484937, 0.55515063]]), array([[0.93154061, 0.06845936]]), array([[0.86578071, 0.13421926]]), array([[0.49594879, 0.50405121]]), array([[0.95584458, 0.04415542]]), array([[0.00845277, 0.99154723]]), array([[0.9911859, 0.0088141]])]\n",
      "테스트\n",
      "씨발\n",
      "새끼\n",
      "새는\n",
      "씨발\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2289, 2699], [6030, 6280], [7071, 7200], [8281, 8480], [8695, 9000]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_swear(ts, stt, stt_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.dataset_train object at 0x7f9b4f443c10>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98576045, 0.01423955]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dependencies = {\n",
    "    'precision': precision,\n",
    "    'recall' : recall,\n",
    "    'f1score' : f1score,\n",
    "  }\n",
    "\n",
    "# model = tf.keras.models.load_model('/Users/2_eu_genie/Desktop/workingspace/grad_web/keras_model_1.h5'\n",
    "#                                     , custom_objects=dependencies)\n",
    "\n",
    "yok_classifier_lime(['이 역은 우리 열차의 시발역입니다.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6dc36e8eb7937989df778c0c8b5e20184c2951adea6369ace3e8b4035810dd53"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
